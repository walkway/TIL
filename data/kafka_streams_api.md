# 카프카 스트림즈 API

### 스트림 프로세싱 기초
- 데이터들이 지속적으로 유입되고 나가는 과정에서 데이터에 대한 분석 or 질의 수행
- 애플리케이션이 이벤트에 즉각 반응. 이벤트 발생 -> 분석 -> 조치 => 최신 데이터 반영.
- 대규모 공유 데이터베이스를 줄임 -> 인프라에 독립적 수행
- 상태 기반: 이전 스트림 처리 결과 참조. ex) 단어 빈도수 세기, 실시간 추천.

### 카프카 스트림즈
- 카프카에 저장된 데이터 처리, 분석을 위한 클라이언트 라이브러리.
- 이벤트 시간과 처리 시간 분리, 다양한 시간 옵션 지원
- 시스템, 카프카에 대한 의존성 x
- 이중화된 로컬 상태 저장소 지원 ???
- 한번에 한 레코드만 처리
- DSL, 프로세싱 API 제공
- 토폴로지: 스트림 처리를 하는 프로세서들이 서로 연결된 형상. 입력 소스로부터 어떤 작업들이 계속 수행되어 최종 결과로 이어짐. 
- 스트림: 카프카 스트림즈 API를 사용해 생성된 토폴로지. 끊임없이 전달되는 데이터 세트. 키-값.
- 프로세스 생성 방법: 카프카 스트림즈 DSL에서 데이터 프로세싱 메소드 제공 / 프로세서 API 제공해서 제수준의 처리 직접 구현.
- 각 스트림 파티션 = 카프카 토픽 파티션에 저장된 정렬 메시지
- 데이터 레코드의 키를 통해 다음 스트림(=카프카 토픽)으로 전달
- 카프카 스트림즈는 입력 스트림의 파티션 개수만큼 태스크 생성, 각 태스크에는 입력 스트림 파티션 할당

````
StreamsBuilder builder = new StreamsBuilder(); // 토폴로지 생성
KStream<String, String> source = builder.stream("streams-plaintext-input"); // 새로운 입력 스트림 생성
source.to("streams-pipe-output"); // 토픽 전달
final Topology topology = builder.build(); // 최종 토폴로지
topology.describe(); // 토폴로지 확인
````
````
final KafkaStreams streams = new KafkaStreams(topology, props); // 스트림즈 애플리케이션 생성
````
- 특정 토픽 내용 읽고 -> 값 변경 -> 새로운 스트림 생성 -> 다른 토픽 전달
- 무상태 오퍼레이터
  - flatMap: 새로운 스트림을 만들 때, 키와 값 모두 새롭게 만들어서 사용
  - flatMapValue: 새로운 스트림을 만들 때, 값만 변경해서 만들어서 사용

### KSQL
- 저장 기간에 관계없이 스트리밍과 배치 처리 동시에 실행
- 람다 아키텍처
  - raw 데이터를 처리해서 기간과 용량에 따라 별도의 저장소를 가지는 것
  - 단기/장기 데이터를 동시에 관리 가능
  - 병목이 생기는 경우 특정 컴포넌트만 증가
  - 단기/장기 데이터 조회 쉬움
  - 단기/장기 데이터 별도 관리 -> 관리 비용 증가
- 카파 아키텍처
  - 데이터의 크기, 기간에 관계없이 하나의 계산 프로그램 사용
  - 장기 데이터 조회가 필요한 경우에 "계산"해서 결과를 그때마다 전달
- KSQL 클라이언트: SQL문을 KSQL 서버에 전달하고 결과를 받는 툴
- 기존 SQL에서 지원하는 테이블 + 연속된 정형화된 데이터를 의미하는 스트림이라는 데이터 모델 제공
- 스트림: 데이터가 계속 기록될 수 있지만, 한번 기록된 이벤트는 변경 x
- 테이블: 이벤트에 따른 현재 상태를 나타냄 -> 변경 o
- https://github.com/confluentinc/ksql
````
// 스트림 생성
CREATE TABLE 스트림이름({컬럼이름 컬럼_데이터_타입}[, ///]) WITH (프로퍼티_이름 = 프로퍼티값 [, ...]);
````
````
BOOLEAN
INTEGER
BIGINT
DOUBLE
VARCHAR
ARRAY<ArrayType> (json만 지원)
MAP<VARCHAR, ValueType> (json만 지원)
````
##### 출처: 카프카 데이터 플랫폼의 최강자